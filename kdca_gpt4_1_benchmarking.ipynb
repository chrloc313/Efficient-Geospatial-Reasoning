{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTvrWzql8N82bz15BU2LvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrloc313/Efficient-Geospatial-Reasoning/blob/main/kdca_gpt4_1_benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "endpoint = userdata.get(\"azure-endpoint\")\n",
        "model_name = \"gpt-4.1\"\n",
        "deployment = \"gpt-4.1\"\n",
        "\n",
        "subscription_key = userdata.get(\"azure-4.1-key\")\n",
        "api_version = \"2024-12-01-preview\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_I1sH7PziTQ8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXLUUrEdLyOw",
        "outputId": "50dc98ad-3425-4196-9eb1-1f512364399b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Efficient-Geospatial-Reasoning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, json\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "curr_dir = os.getcwd()\n",
        "test_dataset_path = curr_dir + \"/data/test_split_dataset.json\"\n",
        "\n",
        "with open(test_dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    test_dataset = json.load(file)\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    image_ids = [item['Image_ID'] for item in batch]\n",
        "    questions = [item['Question'] for item in batch]\n",
        "    ground_truths = [item['Ground_Truth'] for item in batch]\n",
        "    question_types = [item['Question_Type'] for item in batch]\n",
        "\n",
        "    data = {\n",
        "        'Image_ID': image_ids,\n",
        "        'Question': questions,\n",
        "        'Question_Type': question_types\n",
        "    }\n",
        "    target = ground_truths\n",
        "\n",
        "    return data, target\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "T7MvTSl3w80S"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, ground_truth in test_dataloader:\n",
        "    print(f\"Data: {data}\")\n",
        "    print(f\"Answer: {ground_truth}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V51jVYrQ7nW4",
        "outputId": "49cbcef2-75c9-4120-e4a5-1ec44e047897"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: {'Image_ID': ['6640.JPG', '9103.JPG', '8931.JPG', '6964.JPG', '7147.JPG', '6976.JPG', '7720.JPG', '7362.JPG', '7601.JPG', '8404.JPG', '6710.JPG', '7263.JPG', '8149.JPG', '7357.JPG', '8264.JPG', '6348.JPG'], 'Question': ['How many buildings can be seen in this image?', 'Is the entire road flooded?', 'What is the condition of the road in this image?', 'Is the entire road flooded?', 'What is the condition of the road in this image?', 'Is the entire road flooded?', 'How many non flooded buildings can be seen in this image?', 'Is the entire road flooded?', 'What is the overall condition of the given image?', 'How many buildings are non flooded?', 'What is the overall condition of the given image?', 'How many buildings are flooded in this image?', 'What is the condition of the road in this image?', 'How many buildings are flooded?', 'What is the overall condition of the given image?', 'What is the overall condition of the given image?'], 'Question_Type': ['Simple_Counting', 'Yes_No', 'Condition_Recognition', 'Yes_No', 'Condition_Recognition', 'Yes_No', 'Complex_Counting', 'Yes_No', 'Condition_Recognition', 'Complex_Counting', 'Condition_Recognition', 'Complex_Counting', 'Condition_Recognition', 'Complex_Counting', 'Condition_Recognition', 'Condition_Recognition']}\n",
            "Answer: [5, 'No', 'non flooded', 'No', 'non flooded', 'No', 5, 'Yes', 'flooded', 3, 'flooded', 14, 'non flooded', 5, 'non flooded', 'non flooded']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_id):\n",
        "    image_path = os.getcwd() + '/data/images/' + image_id\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "      encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    return encoded_string\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert at flood damage assessment. Your task is to accurately answer questions based on provided image data from the FloodNet dataset. Your responses will be used for benchmarking.\n",
        "\n",
        "For each question, you MUST respond with a JSON object containing a single key, \"answer\", and its corresponding value. Do NOT include any other text, explanations, or conversational filler outside of this JSON object.\n",
        "\n",
        "Here are the rules for the \"answer\" value based on the 'Question_Type':\n",
        "\n",
        "1.  **'Simple_Counting'**: The \"answer\" MUST be an integer value (e.g., `5`).\n",
        "2.  **'Complex_Counting'**: The \"answer\" MUST be an integer value (e.g., `12`).\n",
        "3.  **'Condition_Recognition'**:\n",
        "    *   If the question pertains to the **road**: The \"answer\" MUST be one of 'flooded', 'non flooded', or 'flooded,non flooded'.\n",
        "    *   If the question pertains to the **entire image**: The \"answer\" MUST be one of 'flooded' or 'non flooded'.\n",
        "4.  **'Yes_No'**: The \"answer\" MUST be either 'Yes' or 'No'.\n",
        "\n",
        "If you cannot confidently determine the answer based on the image, the \"answer\" MUST be 'Uncertain'.\n",
        "\n",
        "Respond with the answer choice that you believe most correctly and confidently answers the presented question, strictly adhering to the specified format and possible values.\"\"\"\n",
        "import time\n",
        "\n",
        "def retrieve_response(question, question_type, encoded_string):\n",
        "    start_time = time.perf_counter()\n",
        "    response = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": SYSTEM_PROMPT,\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": [\n",
        "                  {\n",
        "                      \"type\": \"text\",\n",
        "                      \"text\": \"Question Type: {}; Question: {}\".format(question_type, question)\n",
        "                  },\n",
        "                  {\n",
        "                      \"type\": \"image_url\",\n",
        "                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_string}\"}\n",
        "                  },\n",
        "              ],\n",
        "          }\n",
        "      ],\n",
        "      max_completion_tokens=13107,\n",
        "      temperature=1.0,\n",
        "      top_p=1.0,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0,\n",
        "      model=deployment\n",
        "    )\n",
        "\n",
        "    return {\"Response\": response.choices[0].message.content, \"Elapsed_Time\": time.perf_counter() - start_time}\n",
        "\n",
        "def answer_evalulation(model_response, ground_truth):\n",
        "    try:\n",
        "        answer = json.loads(model_response)\n",
        "        if isinstance(answer, dict) and 'answer' in answer:\n",
        "            if answer['answer'] == ground_truth:\n",
        "                return 1\n",
        "        return 0\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Warning: Model returned non-JSON response: {model_response}\")\n",
        "        return 0"
      ],
      "metadata": {
        "id": "qkbaYfNp84Wc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(test_dataset)\n",
        "df['Model_Response'] = pd.Series(dtype='object')\n",
        "df['Correct?'] = pd.Series(dtype='bool')\n",
        "df['Model_Response_Time'] = pd.Series(dtype='float64')\n",
        "rolling_score = 0\n",
        "\n",
        "for i, (batch_data, batch_answers) in enumerate(test_dataloader):\n",
        "    current_batch_size = len(batch_data['Image_ID'])\n",
        "    encoded_images = [encode_image(batch_data['Image_ID'][k]) for k in range(current_batch_size)]\n",
        "    model_responses = [retrieve_response(batch_data[\"Question\"][k], batch_data[\"Question_Type\"][k], encoded_images[k]) for k in range(current_batch_size)]\n",
        "    results = [answer_evalulation(model_responses[k][\"Response\"], batch_answers[k]) for k in range(current_batch_size)]\n",
        "\n",
        "    for j in range(current_batch_size):\n",
        "        row = i * BATCH_SIZE + j\n",
        "        df.at[row, 'Model_Response'] = model_responses[j][\"Response\"]\n",
        "        df.at[row, 'Correct?'] = bool(results[j])\n",
        "        df.at[row, 'Model_Response_Time'] = model_responses[j][\"Elapsed_Time\"]\n",
        "    rolling_score += sum(results)\n",
        "    print(f\"Batch {i}/{len(test_dataloader)} finished\")\n",
        "\n",
        "new_df_order = ['Question', 'Question_Type', 'Correct?', 'Ground_Truth', 'Model_Response', 'Model_Response_Time', 'Image_ID']\n",
        "df = df.reindex(columns=new_df_order)\n",
        "\n",
        "benchmark_output_path = os.getcwd() + '/data/benchmarks'\n",
        "os.makedirs(os.path.dirname(benchmark_output_path), exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "while True:\n",
        "    file_path = benchmark_output_path + '/gpt4.1-benchmark-output_{}.csv'.format(i)\n",
        "    if not os.path.exists(file_path):\n",
        "        df.to_csv(file_path, index=False)\n",
        "        break\n",
        "    i += 1\n",
        "\n",
        "print(f\"Final Accuracy: {rolling_score/len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "wHdexrWew1IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2dd3d9a-00ce-4dff-a65e-0257328b8928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/43 finished\n",
            "Batch 1/43 finished\n",
            "Batch 2/43 finished\n",
            "Batch 3/43 finished\n",
            "Batch 4/43 finished\n",
            "Batch 5/43 finished\n",
            "Batch 6/43 finished\n"
          ]
        }
      ]
    }
  ]
}